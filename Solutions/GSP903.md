### GSP903 : Stream Processing with Cloud Pub/Sub and Dataflow: Qwik Start


## Run in Cloudshell
```cmd
REGION=us-east1
```
```cmd
PROJECT_ID=$(gcloud config get-value project)
BUCKET_NAME="${PROJECT_ID}-bucket"
TOPIC_ID=my-id
gcloud config set compute/region $REGION
gcloud services disable dataflow.googleapis.com
gcloud services enable dataflow.googleapis.com
gsutil mb gs://$BUCKET_NAME
gcloud pubsub topics create $TOPIC_ID
gcloud app create --region=$REGION
echo "y" | gcloud scheduler jobs create pubsub publisher-job \
--schedule="* * * * *" \
--topic=$TOPIC_ID \
--message-body="Hello!"
gcloud scheduler jobs run publisher-job
docker run -it -e DEVSHELL_PROJECT_ID=$DEVSHELL_PROJECT_ID -e BUCKET_NAME=$BUCKET_NAME -e PROJECT_ID=$PROJECT_ID -e REGION=$REGION -e TOPIC_ID=$TOPIC_ID python:3.7 /bin/bash
```

Paste into docker container
```cmd
git clone https://github.com/GoogleCloudPlatform/python-docs-samples.git
cd python-docs-samples/pubsub/streaming-analytics
pip install -U -r requirements.txt
python PubSubToGCS.py \
--project=$PROJECT_ID \
--region=$REGION \
--input_topic=projects/$PROJECT_ID/topics/$TOPIC_ID \
--output_path=gs://$BUCKET_NAME/samples/output \
--runner=DataflowRunner \
--window_size=2 \
--num_shards=2 \
--temp_location=gs://$BUCKET_NAME/temp
```
